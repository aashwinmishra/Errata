{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMM+ShtEuauJSS3fVlMdW77"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LHD6kr1b3pik"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re"
      ],
      "metadata": {
        "id": "6ftLOSWi_4N0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtkgfOgJAA3c",
        "outputId": "0d30e19d-4bcb-496c-bf7c-b63e9a3c095e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpuyOEl5ACOw",
        "outputId": "b4da282e-ddcf-4a01-d3f1-681ca7aa2f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(len(result))\n",
        "print(len(np.unique(result)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHClSulEDsbU",
        "outputId": "c65b27aa-ebc8-47fd-dfe3-eba9d97c18cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n",
            "1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = np.sort(np.unique([word for word in result]))\n",
        "word_2_idx = {w:v for v,w in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "vqVk5qcTFITO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer_V1:\n",
        "  def __init__(self, vocab: list):\n",
        "    self.word2idx = {w:v for v,w in enumerate(vocab)}\n",
        "    self.word2idx[\"<unk>\"] = len(vocab) + 1\n",
        "    self.word2idx[\"<eot>\"] = self.word2idx[\"<unk>\"] +1\n",
        "    self.idx2word = {v:k for k,v in self.word2idx.items()}\n",
        "\n",
        "  def encode(self, text: str):\n",
        "    result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    result = [item.strip() for item in result if item.strip()]\n",
        "    return [self.word2idx[token] if token in self.word2idx else self.word2idx[\"<unk>\"] for token in result] + [self.word2idx[\"<eot>\"]]\n",
        "\n",
        "  def decode(self, ids: list):\n",
        "    text = \" \".join([self.idx2word[id] for id in ids])\n",
        "    text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "n1Ref-hXAM0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am a loser.\"\n",
        "tokenizer = Tokenizer_V1(vocab)\n",
        "ids = tokenizer.encode(text)\n",
        "words = tokenizer.decode(ids)\n",
        "print(text)\n",
        "print(ids)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA_YIcL0Ay7H",
        "outputId": "e84cd54f-4620-463d-9c3e-374b85cdf3c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am a loser.\n",
            "[53, 150, 115, 1131, 7, 1132]\n",
            "I am a <unk>. <eot>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "B9ks4_NFMUYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPYjBuASEQiL",
        "outputId": "4462d64c-b748-4191-b807-342e3a0f176b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_chunks(encoded: list, context_length: int):\n",
        "  inputs = []\n",
        "  targets = []\n",
        "  num_chunks = (len(encoded) - 1) // context_length\n",
        "  for i in range(num_chunks):\n",
        "    input, target = encoded[i*context_length: (i+1)*context_length], encoded[i*context_length + 1: (i+1)*context_length + 1]\n",
        "    inputs.append(input)\n",
        "    targets.append(target)\n",
        "\n",
        "  return torch.tensor(inputs), torch.tensor(targets)"
      ],
      "metadata": {
        "id": "_Zr7XdOQGrFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = create_data_chunks(enc_text, 12)\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print(X[0])\n",
        "print(Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qriEX-j6-HHL",
        "outputId": "9c808a2e-b198-4ae1-a188-5b8c3a78f406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([428, 12])\n",
            "torch.Size([428, 12])\n",
            "tensor([   40,   367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,\n",
            "          257,  7026])\n",
            "tensor([  367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,   257,\n",
            "         7026, 15632])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, text, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids  = []\n",
        "\n",
        "    token_ids = tokenizer.encode(text)\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "      inputs, outputs = token_ids[i: i+max_length], token_ids[i+1: i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(inputs))\n",
        "      self.target_ids.append(torch.tensor(outputs))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "MS1TrSlQ-SmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(text, tokenizer, max_length, stride, batch_size, shuffle=True, drop_last=True):\n",
        "  dataset = GPTDataset(text, tokenizer,  max_length, stride)\n",
        "  return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)"
      ],
      "metadata": {
        "id": "64Qau54PEKLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader(raw_text, tokenizer, batch_size=4, max_length=32, stride=1, shuffle=False)"
      ],
      "metadata": {
        "id": "Yze_aCtgEtKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = next(iter(dataloader))\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACnUH4BZE3xy",
        "outputId": "6a4601b4-d1ea-4ce5-aedc-94f1957d897e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb_layer = torch.nn.Embedding(num_embeddings=tokenizer.n_vocab, embedding_dim=16)"
      ],
      "metadata": {
        "id": "DRM8wfTeE9-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = emb_layer(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkHoSBRsHGeH",
        "outputId": "ded89f18-245e-4f99-df89-033a84a3c749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 32, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_setup.py\n",
        "import torch\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "class GPTDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,\n",
        "               txt: str,\n",
        "               tokenizer: tiktoken.Encoding,\n",
        "               max_length: int,\n",
        "               stride: int):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "    token_ids = tokenizer.encode(txt)\n",
        "\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "      input_chunk = token_ids[i: i+max_length]\n",
        "      target_chunk = token_ids[i+1: i+1+max_length]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader(txt, tokenizer, max_length=256, stride=128, batch_size=4, shuffle=True, drop_last=True, num_workers=0):\n",
        "  dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
        "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "  return dataloader\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRPQpBS6HI33",
        "outputId": "6a05ae5d-b9ee-4a35-d6e2-daef457dee9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = 32\n",
        "embedding_dim = 16\n",
        "\n",
        "emb_layer = torch.nn.Embedding(num_embeddings=tokenizer.n_vocab, embedding_dim=embedding_dim)\n",
        "pos_encoding_layer = torch.nn.Embedding(context_length, embedding_dim)\n",
        "pos_enc = pos_encoding_layer(torch.arange(context_length))\n",
        "\n",
        "out = emb_layer(x) + pos_enc\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5vS4ymuRwr0",
        "outputId": "da7da92f-896d-4ad0-a9dd-b9f8667905fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 32, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "[[0.43, 0.15, 0.89], # Your (x^1)\n",
        "[0.55, 0.87, 0.66], # journey (x^2)\n",
        "[0.57, 0.85, 0.64], # starts (x^3)\n",
        "[0.22, 0.58, 0.33], # with (x^4)\n",
        "[0.77, 0.25, 0.10], # one (x^5)\n",
        "[0.05, 0.80, 0.55]] # step (x^6)\n",
        ")"
      ],
      "metadata": {
        "id": "PgciZ_lrTpsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmUIQmJzD5t1",
        "outputId": "d80c96fa-097a-4e65-cfcb-09d4891628d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WCnnubccD8Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hLJSCFJMFNwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionV1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    d = torch.tensor(inputs.shape[-1])\n",
        "    Q, K, V = inputs, inputs, inputs\n",
        "    attention_scores = Q @ K.transpose(-1, -2)/torch.sqrt(d)\n",
        "    attention_weights = torch.softmax(attention_scores, dim=-1)\n",
        "    return attention_weights @ V"
      ],
      "metadata": {
        "id": "k3V3l06oFhm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionV2(nn.Module):\n",
        "  def __init__(self, d: int, qkv_bias: bool=False):\n",
        "    super().__init__()\n",
        "    self.d = torch.tensor(d)\n",
        "    self.Wq = nn.Linear(d, d, bias=qkv_bias)\n",
        "    self.Wk = nn.Linear(d, d, bias=qkv_bias)\n",
        "    self.Wv = nn.Linear(d, d, bias=qkv_bias)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    Q = self.Wq(inputs)\n",
        "    K = self.Wk(inputs)\n",
        "    V = self.Wv(inputs)\n",
        "    attention_scores = Q @ K.transpose(-1, -2)/torch.sqrt(self.d)\n",
        "    attention_weights = torch.softmax(attention_scores, dim=-1)\n",
        "    return attention_weights @ V"
      ],
      "metadata": {
        "id": "rCpXEedWPQ1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self,\n",
        "               context_length: int,\n",
        "               d: int,\n",
        "               dropout_rate: float=0.1,\n",
        "               qkv_bias: bool=False) -> None:\n",
        "    super().__init__()\n",
        "    self.d = torch.tensor(d)\n",
        "    self.Wq = nn.Linear(d, d, bias=qkv_bias)\n",
        "    self.Wk = nn.Linear(d, d, bias=qkv_bias)\n",
        "    self.Wv = nn.Linear(d, d, bias=qkv_bias)\n",
        "    self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    num_tokens = inputs.shape[-2]\n",
        "    Q = self.Wq(inputs)\n",
        "    K = self.Wk(inputs)\n",
        "    V = self.Wv(inputs)\n",
        "    attention_scores = Q @ K.transpose(-1, -2)/torch.sqrt(self.d)\n",
        "    attention_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) #to account for shorter sequences\n",
        "    attention_weights = torch.softmax(attention_scores, dim=-1)\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "    print(attention_weights)\n",
        "    return attention_weights @ V\n",
        "\n"
      ],
      "metadata": {
        "id": "HTOr07KtRWkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAtttention(nn.Module):\n",
        "  def __init__(self,\n",
        "               context_length: int,\n",
        "               embed_dim: int,\n",
        "               num_heads: int,\n",
        "               dropout_rate: float,\n",
        "               qkv_bias=False):\n",
        "    assert embed_dim % num_heads == 0\n",
        "    super().__init__()\n",
        "    self.d = torch.tensor(embed_dim)\n",
        "    self.Wq = nn.Linear(embed_dim, embed_dim, bias=qkv_bias)\n",
        "    self.Wk = nn.Linear(embed_dim, embed_dim, bias=qkv_bias)\n",
        "    self.Wv = nn.Linear(embed_dim, embed_dim, bias=qkv_bias)\n",
        "    self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.dh = int(embed_dim / num_heads)\n",
        "    self.num_heads = num_heads\n",
        "    self.Wo = nn.Linear(embed_dim, embed_dim, bias=qkv_bias)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    batch_size, num_tokens, embed_dim = inputs.shape\n",
        "    Q = self.Wq(inputs) #shape: [B, C, dm]\n",
        "    K = self.Wk(inputs) #shape: [B, C, dm]\n",
        "    V = self.Wv(inputs) #shape: [B, C, dm]\n",
        "    Q = Q.reshape(Q.shape[0], Q.shape[1], self.num_heads, self.dh).transpose(-2, -3) #shape: [B, nh, C, dk]\n",
        "    K = K.reshape(K.shape[0], K.shape[1], self.num_heads, self.dh).transpose(-2, -3) #shape: [B, nh, C, dk]\n",
        "    V = V.reshape(V.shape[0], V.shape[1], self.num_heads, self.dh).transpose(-2, -3) #shape: [B, nh, C, dk]\n",
        "    attention_scores = Q @ K.transpose(-1, -2) / (self.dh)**0.5\n",
        "    attention_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "    attention_weights = torch.softmax(attention_scores, dim=-1)\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "    context_matrix = attention_weights @ V\n",
        "    context_matrix = context_matrix.transpose(-2, -3).reshape(context_matrix.shape[0], context_matrix.shape[2], self.d)\n",
        "    return self.Wo(context_matrix)"
      ],
      "metadata": {
        "id": "IHYDJlQb9WXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = MultiHeadAtttention(context_length=16, embed_dim=32, num_heads=4, dropout_rate=0.5)\n",
        "input = torch.rand(1, 16, 32)\n",
        "out = l(input)\n",
        "print(input.shape)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVb7oDHMG5zh",
        "outputId": "a95872cd-2444-4006-c8c0-1453b145d980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 16, 32])\n",
            "torch.Size([1, 16, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"embed_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}"
      ],
      "metadata": {
        "id": "N8a3buNnHJGs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in GPT_CONFIG_124M.items():\n",
        "  print(f\"{k}: \\t {v}\")"
      ],
      "metadata": {
        "id": "cleOMSpyZ6Gl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "906e0141-8e64-490f-9cb5-a5d99fb437ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size: \t 50257\n",
            "context_length: \t 1024\n",
            "embed_dim: \t 768\n",
            "n_heads: \t 12\n",
            "n_layers: \t 12\n",
            "drop_rate: \t 0.1\n",
            "qkv_bias: \t False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z6aKZkt0ZqEM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aKQRDGdcOzv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fakm36Er3mnO",
        "outputId": "3e58b6ba-cb3c-4d7b-87c5-59f81363b684"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FIOInzql45Dx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qUSmWE0D4_oY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, normalized_shape: int=768, eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.scale = nn.Parameter(torch.ones(normalized_shape))\n",
        "    self.shift = nn.Parameter(torch.zeros(normalized_shape))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.scale * (x - torch.mean(x, dim=-1, keepdim=True)) / \\\n",
        "    torch.sqrt(torch.var(x, dim=-1, keepdim=True) + self.eps) + self.shift"
      ],
      "metadata": {
        "id": "09tBC6tY2Zho"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeLU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1.0 + torch.tanh((2/torch.pi)**0.5 * \\\n",
        "     (x + 0.044715*torch.pow(x, 3))))"
      ],
      "metadata": {
        "id": "A7-MMLbpQKQH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ln = LayerNorm(512)\n",
        "g = GeLU()\n",
        "with torch.inference_mode():\n",
        "  out = ln(dummy_batch)\n",
        "  out = g(out)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfNo7rzx66Ai",
        "outputId": "f0e50c00-55e8-458d-d783-6b9ab490499a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 256, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MmmztQwg7GtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"embed_dim\"])\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"embed_dim\"])\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "        *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "    )\n",
        "    self.final_norm = LayerNorm(cfg[\"embed_dim\"])\n",
        "    self.out_head = nn.Linear(cfg[\"embed_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, normalized_shape: int=768, eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.scale = nn.Parameter(torch.ones(normalized_shape))\n",
        "    self.shift = nn.Parameter(torch.zeros(normalized_shape))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.scale * (x - torch.mean(x, dim=-1, keepdim=True)) / \\\n",
        "    torch.sqrt(torch.var(x, dim=-1, keepdim=True) + self.eps) + self.shift\n",
        "\n",
        "\n",
        "class GeLU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1.0 + torch.tanh((2/torch.pi)**0.5 * \\\n",
        "     (x + 0.044715*torch.pow(x, 3))))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg[\"embed_dim\"], 4*cfg[\"embed_dim\"]),\n",
        "        GeLU(),\n",
        "        nn.Linear(4*cfg[\"embed_dim\"], cfg[\"embed_dim\"])\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out,\n",
        "               context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert (d_out % num_heads == 0), \\\n",
        "    \"d_out must be divisible by num_heads\"\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_out, d_out)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.mask = torch.triu(torch.ones(context_length, context_length),diagonal=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    keys = keys.transpose(1, 2)\n",
        "    queries = queries.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "    attn_scores = queries @ keys.transpose(2, 3)\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "    attn_weights = torch.softmax(\n",
        "    attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "    context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
        "    context_vec = self.out_proj(context_vec)\n",
        "    return context_vec\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg) -> None:\n",
        "    super().__init__()\n",
        "    self.att = MultiHeadAttention(d_in=cfg[\"embed_dim\"],\n",
        "                                  d_out=cfg[\"embed_dim\"],\n",
        "                                  context_length=cfg[\"context_length\"],\n",
        "                                  dropout=cfg[\"drop_rate\"],\n",
        "                                  num_heads=cfg[\"n_heads\"],\n",
        "                                  qkv_bias=cfg[\"qkv_bias\"])\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.norm1 = LayerNorm(cfg[\"embed_dim\"])\n",
        "    self.norm2 = LayerNorm(cfg[\"embed_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "    skip = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + skip\n",
        "    skip = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + skip\n",
        "    return x\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAt6dlK78Rla",
        "outputId": "28724ff3-fc2e-4fcf-d6f0-ba23f4a57c31"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from models import TransformerBlock, GPTModel"
      ],
      "metadata": {
        "id": "5hD527cFgDhR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "x = torch.rand(2, 4, 768)\n",
        "block = TransformerBlock(GPT_CONFIG_124M)\n",
        "output = block(x)"
      ],
      "metadata": {
        "id": "vS7ecVEEtrww"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1spuMzztufY",
        "outputId": "3b7cef43-2aa2-48b9-935c-0c083bb9c259"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 4, 768])\n",
            "Output shape: torch.Size([2, 4, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M8HZUeatw5M",
        "outputId": "e2a0b861-6755-4f56-aa9d-42ad42ad7f5d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[ 0.1381,  0.0079, -0.1957,  ..., -0.0222, -0.1062,  0.1717],\n",
            "         [ 0.3867, -0.8400, -0.6558,  ..., -0.5162,  0.2362, -0.3349],\n",
            "         [ 0.6985, -0.1826, -0.1634,  ...,  0.1472, -0.6503, -0.0054],\n",
            "         [-0.4288,  0.1670, -0.1262,  ...,  1.1571,  0.5297, -0.5542]],\n",
            "\n",
            "        [[ 0.1095, -0.2890, -0.1463,  ..., -0.0557,  0.2907, -0.2818],\n",
            "         [ 0.0884, -0.3545, -0.3524,  ...,  1.2921,  0.0050,  0.1902],\n",
            "         [ 0.6092,  0.4702, -0.4093,  ...,  0.7682,  0.3781, -0.1968],\n",
            "         [-0.0608, -0.0739,  0.4747,  ...,  1.2458, -0.3834,  0.0612]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z9lXbChCu5TT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}