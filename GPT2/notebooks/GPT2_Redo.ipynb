{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7fNx9kLIh5JbsWv371PhS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHD6kr1b3pik",
        "outputId": "c93d7b27-44d1-409c-ed51-13d7e30684f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.2 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re"
      ],
      "metadata": {
        "id": "6ftLOSWi_4N0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtkgfOgJAA3c",
        "outputId": "0d30e19d-4bcb-496c-bf7c-b63e9a3c095e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpuyOEl5ACOw",
        "outputId": "b4da282e-ddcf-4a01-d3f1-681ca7aa2f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(len(result))\n",
        "print(len(np.unique(result)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHClSulEDsbU",
        "outputId": "c65b27aa-ebc8-47fd-dfe3-eba9d97c18cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n",
            "1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = np.sort(np.unique([word for word in result]))\n",
        "word_2_idx = {w:v for v,w in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "vqVk5qcTFITO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer_V1:\n",
        "  def __init__(self, vocab: list):\n",
        "    self.word2idx = {w:v for v,w in enumerate(vocab)}\n",
        "    self.word2idx[\"<unk>\"] = len(vocab) + 1\n",
        "    self.word2idx[\"<eot>\"] = self.word2idx[\"<unk>\"] +1\n",
        "    self.idx2word = {v:k for k,v in self.word2idx.items()}\n",
        "\n",
        "  def encode(self, text: str):\n",
        "    result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    result = [item.strip() for item in result if item.strip()]\n",
        "    return [self.word2idx[token] if token in self.word2idx else self.word2idx[\"<unk>\"] for token in result] + [self.word2idx[\"<eot>\"]]\n",
        "\n",
        "  def decode(self, ids: list):\n",
        "    text = \" \".join([self.idx2word[id] for id in ids])\n",
        "    text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "n1Ref-hXAM0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am a loser.\"\n",
        "tokenizer = Tokenizer_V1(vocab)\n",
        "ids = tokenizer.encode(text)\n",
        "words = tokenizer.decode(ids)\n",
        "print(text)\n",
        "print(ids)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA_YIcL0Ay7H",
        "outputId": "e84cd54f-4620-463d-9c3e-374b85cdf3c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am a loser.\n",
            "[53, 150, 115, 1131, 7, 1132]\n",
            "I am a <unk>. <eot>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "B9ks4_NFMUYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPYjBuASEQiL",
        "outputId": "4462d64c-b748-4191-b807-342e3a0f176b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_chunks(encoded: list, context_length: int):\n",
        "  inputs = []\n",
        "  targets = []\n",
        "  num_chunks = (len(encoded) - 1) // context_length\n",
        "  for i in range(num_chunks):\n",
        "    input, target = encoded[i*context_length: (i+1)*context_length], encoded[i*context_length + 1: (i+1)*context_length + 1]\n",
        "    inputs.append(input)\n",
        "    targets.append(target)\n",
        "\n",
        "  return torch.tensor(inputs), torch.tensor(targets)"
      ],
      "metadata": {
        "id": "_Zr7XdOQGrFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = create_data_chunks(enc_text, 12)\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print(X[0])\n",
        "print(Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qriEX-j6-HHL",
        "outputId": "9c808a2e-b198-4ae1-a188-5b8c3a78f406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([428, 12])\n",
            "torch.Size([428, 12])\n",
            "tensor([   40,   367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,\n",
            "          257,  7026])\n",
            "tensor([  367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,   257,\n",
            "         7026, 15632])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, text, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids  = []\n",
        "\n",
        "    token_ids = tokenizer.encode(text)\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "      inputs, outputs = token_ids[i: i+max_length], token_ids[i+1: i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(inputs))\n",
        "      self.target_ids.append(torch.tensor(outputs))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "MS1TrSlQ-SmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(text, tokenizer, max_length, stride, batch_size, shuffle=True, drop_last=True):\n",
        "  dataset = GPTDataset(text, tokenizer,  max_length, stride)\n",
        "  return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)"
      ],
      "metadata": {
        "id": "64Qau54PEKLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader(raw_text, tokenizer, batch_size=4, max_length=32, stride=1, shuffle=False)"
      ],
      "metadata": {
        "id": "Yze_aCtgEtKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = next(iter(dataloader))\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACnUH4BZE3xy",
        "outputId": "6a4601b4-d1ea-4ce5-aedc-94f1957d897e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb_layer = torch.nn.Embedding(num_embeddings=tokenizer.n_vocab, embedding_dim=16)"
      ],
      "metadata": {
        "id": "DRM8wfTeE9-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = emb_layer(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkHoSBRsHGeH",
        "outputId": "ded89f18-245e-4f99-df89-033a84a3c749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 32, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_setup.py\n",
        "import torch\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "class GPTDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,\n",
        "               txt: str,\n",
        "               tokenizer: tiktoken.Encoding,\n",
        "               max_length: int,\n",
        "               stride: int):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "    token_ids = tokenizer.encode(txt)\n",
        "\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "      input_chunk = token_ids[i: i+max_length]\n",
        "      target_chunk = token_ids[i+1: i+1+max_length]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader(txt, tokenizer, max_length=256, stride=128, batch_size=4, shuffle=True, drop_last=True, num_workers=0):\n",
        "  dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
        "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "  return dataloader\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRPQpBS6HI33",
        "outputId": "6a05ae5d-b9ee-4a35-d6e2-daef457dee9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = 32\n",
        "embedding_dim = 16\n",
        "\n",
        "emb_layer = torch.nn.Embedding(num_embeddings=tokenizer.n_vocab, embedding_dim=embedding_dim)\n",
        "pos_encoding_layer = torch.nn.Embedding(context_length, embedding_dim)\n",
        "pos_enc = pos_encoding_layer(torch.arange(context_length))\n",
        "\n",
        "out = emb_layer(x) + pos_enc\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5vS4ymuRwr0",
        "outputId": "da7da92f-896d-4ad0-a9dd-b9f8667905fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 32, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "[[0.43, 0.15, 0.89], # Your (x^1)\n",
        "[0.55, 0.87, 0.66], # journey (x^2)\n",
        "[0.57, 0.85, 0.64], # starts (x^3)\n",
        "[0.22, 0.58, 0.33], # with (x^4)\n",
        "[0.77, 0.25, 0.10], # one (x^5)\n",
        "[0.05, 0.80, 0.55]] # step (x^6)\n",
        ")"
      ],
      "metadata": {
        "id": "PgciZ_lrTpsx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmUIQmJzD5t1",
        "outputId": "d80c96fa-097a-4e65-cfcb-09d4891628d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WCnnubccD8Kz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hLJSCFJMFNwH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionV1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    d = torch.tensor(inputs.shape[-1])\n",
        "    Q, K, V = inputs, inputs, inputs\n",
        "    attention_scores = Q @ K.transpose(-1, -2)/torch.sqrt(d)\n",
        "    attention_weights = torch.softmax(attention_scores, dim=-1)\n",
        "    return attention_weights @ V"
      ],
      "metadata": {
        "id": "k3V3l06oFhm_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionV2(nn.Module):\n",
        "  def __init__(self, d: int, qkv_bias: bool=False):\n",
        "    super().__init__()\n",
        "    self.d = torch.tensor(d)\n",
        "    self.Wq = nn.Linear(d, d, bias=qkv_bias)\n",
        "    self.Wk = nn.Linear(d, d, bias=qkv_bias)\n",
        "    self.Wv = nn.Linear(d, d, bias=qkv_bias)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    Q = self.Wq(inputs)\n",
        "    K = self.Wk(inputs)\n",
        "    V = self.Wv(inputs)\n",
        "    attention_scores = Q @ K.transpose(-1, -2)/torch.sqrt(self.d)\n",
        "    attention_weights = torch.softmax(attention_scores, dim=-1)\n",
        "    return attention_weights @ V"
      ],
      "metadata": {
        "id": "rCpXEedWPQ1H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self,\n",
        "               context_length: int,\n",
        "               d: int,\n",
        "               dropout_rate: float=0.1,\n",
        "               qkv_bias: bool=False) -> None:\n",
        "    super().__init__()\n",
        "    self.d = torch.tensor(d)\n",
        "    self.Wq = nn.Linear(d, d, bias=qkv_bias)\n",
        "    self.Wk = nn.Linear(d, d, bias=qkv_bias)\n",
        "    self.Wv = nn.Linear(d, d, bias=qkv_bias)\n",
        "    self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    num_tokens = inputs.shape[-2]\n",
        "    Q = self.Wq(inputs)\n",
        "    K = self.Wk(inputs)\n",
        "    V = self.Wv(inputs)\n",
        "    attention_scores = Q @ K.transpose(-1, -2)/torch.sqrt(self.d)\n",
        "    attention_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) #to account for shorter sequences\n",
        "    attention_weights = torch.softmax(attention_scores, dim=-1)\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "    print(attention_weights)\n",
        "    return attention_weights @ V\n",
        "\n"
      ],
      "metadata": {
        "id": "HTOr07KtRWkM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAtttention(nn.Module):\n",
        "  def __init__(self,\n",
        "               context_length: int,\n",
        "               embed_dim: int,\n",
        "               num_heads: int,\n",
        "               dropout_rate: float,\n",
        "               qkv_bias=False):\n",
        "    assert embed_dim % num_heads == 0\n",
        "    super().__init__()\n",
        "    self.d = torch.tensor(embed_dim)\n",
        "    self.Wq = nn.Linear(embed_dim, embed_dim, bias=qkv_bias)\n",
        "    self.Wk = nn.Linear(embed_dim, embed_dim, bias=qkv_bias)\n",
        "    self.Wv = nn.Linear(embed_dim, embed_dim, bias=qkv_bias)\n",
        "    self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.dh = int(embed_dim / num_heads)\n",
        "    self.num_heads = num_heads\n",
        "    self.Wo = nn.Linear(embed_dim, embed_dim, bias=qkv_bias)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    batch_size, num_tokens, embed_dim = inputs.shape\n",
        "    Q = self.Wq(inputs) #shape: [B, C, dm]\n",
        "    K = self.Wk(inputs) #shape: [B, C, dm]\n",
        "    V = self.Wv(inputs) #shape: [B, C, dm]\n",
        "    Q = Q.reshape(Q.shape[0], Q.shape[1], self.num_heads, self.dh).transpose(-2, -3) #shape: [B, nh, C, dk]\n",
        "    K = K.reshape(K.shape[0], K.shape[1], self.num_heads, self.dh).transpose(-2, -3) #shape: [B, nh, C, dk]\n",
        "    V = V.reshape(V.shape[0], V.shape[1], self.num_heads, self.dh).transpose(-2, -3) #shape: [B, nh, C, dk]\n",
        "    attention_scores = Q @ K.transpose(-1, -2) / (self.dh)**0.5\n",
        "    attention_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "    attention_weights = torch.softmax(attention_scores, dim=-1)\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "    context_matrix = attention_weights @ V\n",
        "    context_matrix = context_matrix.transpose(-2, -3).reshape(context_matrix.shape[0], context_matrix.shape[2], self.d)\n",
        "    return self.Wo(context_matrix)"
      ],
      "metadata": {
        "id": "IHYDJlQb9WXX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = MultiHeadAtttention(context_length=16, embed_dim=32, num_heads=4, dropout_rate=0.5)\n",
        "input = torch.rand(1, 16, 32)\n",
        "out = l(input)\n",
        "print(input.shape)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVb7oDHMG5zh",
        "outputId": "a95872cd-2444-4006-c8c0-1453b145d980"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 16, 32])\n",
            "torch.Size([1, 16, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N8a3buNnHJGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cleOMSpyZ6Gl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}